{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universidad Nacional de Córdoba\n",
    "# DIPLODATOS - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "## Trabajo Práctico Número 1 (Analisis del archivo exercise_1.py)\n",
    "\n",
    "Autores: \n",
    "* Diaz Cobos Facundo\n",
    "* Epifanio Luis\n",
    "* Gonzalez Leonardo David\n",
    "* Gualpa Mariano Martín\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Consigna del Ejercicio 1.\n",
    "\n",
    "1. Procesar el conjunto de datos para obtener una representación TfIDf de cada review.\n",
    "  * Hint: User el código de *Aprendizaje Supervisado*\n",
    "  * Hint: El método de fit de Keras crea internamente un conjunto de datos de validación, por lo que no tienen que preocuparse por eso.\n",
    "\n",
    "2. Construir un pipeline de clasificación con un modelo Keras MLP.\n",
    "\n",
    "3. Entrenar uno o varios modelos (con dos o tres es suficiente, veremos más de esto en el práctico 2). Evaluar los modelos en el conjunto de test.\n",
    "\n",
    "4. Reportar los hyperparámetros y resultados de todos los modelos entrenados. Para esto, pueden utilizar una notebook o un archivo (pdf|md). Dentro de este reporte tiene que describir:\n",
    "  * Hyperparámetros con los que procesaron el dataset: tamaño del vocabulario, normalizaciones, etc.\n",
    "  * Las decisiones tomadas al construir cada modelo: regularización, dropout, número y tamaño de las capas, optimizador.\n",
    "  * Proceso de entrenamiento: división del train/test, tamaño del batch, número de épocas, métricas de evaluación. Seleccione los mejores hiperparámetros en función de su rendimiento. El proceso de entrenamiento debería ser el mismo para todos los modelos.\n",
    "  * (Punto estrella) Analizar si el clasificador está haciendo overfitting. Esto se puede determinar a partir del resultado del método fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Desarrollo.\n",
    "\n",
    "## 2.1. Carga de Librerías y Funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:02.949218Z",
     "start_time": "2018-09-22T03:45:02.936300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.613427Z",
     "start_time": "2018-09-22T03:45:02.951091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgualpa/anaconda3/envs/DataScience/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "import argparse\n",
    "import pandas\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.utils import np_utils   # for tf 1.3.1\n",
    "#from tensorflow.python.keras import utils as np_utils     # for tf 1.4.1\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import repeat\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from printutils import print_message, print_new_process, print_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.637078Z",
     "start_time": "2018-09-22T03:45:04.616162Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_args():\n",
    "    parser = argparse.ArgumentParser(description='Exercise 1')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "    parser.add_argument('--num_units', nargs='+', default=[100], type=int,\n",
    "                        help='Number of hidden units of each hidden layer.')\n",
    "    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "                        help='Dropout ratio for every layer.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Number of instances in each batch.')\n",
    "\n",
    "    # New parameters:    \n",
    "    parser.add_argument('--model', type=int, default=10, help='Number of model to run')\n",
    "    parser.add_argument('--max_features', type=int, default=2000, help='Max number of words used inTfidfVectorizer')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')\n",
    "    parser.add_argument('--shuffle', type=str, default='batch', help='Shuffle value')\n",
    "    parser.add_argument('--random_seed', type=int, default=10, help='Random seed number')\n",
    "    parser.add_argument('--verbose', type=int, default=1, help='Verbose info on screen')\n",
    "    \n",
    "    # parse parameters\n",
    "    if arguments == None:\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        args = parser.parse_args(arguments)\n",
    "\n",
    "    assert len(args.num_units) == len(args.dropout)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Procesar el conjunto de datos utilizando TfIDf vectorizer para crear una matriz de entrada.\n",
    "\n",
    "__Responde al punto:__\n",
    "\n",
    "_1. Procesar el conjunto de datos para obtener una representación TfIDf de cada review._\n",
    "  * _Hint: User el código de *Aprendizaje Supervisado*_\n",
    "  * _Hint: El método de fit de Keras crea internamente un conjunto de datos de validación, por lo que no tienen que preocuparse por eso._\n",
    "\n",
    "\n",
    "### Apply the Tfidf vectorizer to create input matrix\n",
    "Creamos un método para vectorizar la entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.661331Z",
     "start_time": "2018-09-22T03:45:04.639663Z"
    }
   },
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "# TODO 1: Apply the Tfidf vectorizer to create input matrix\n",
    "def vectorize_input(x_train, x_test,args):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', use_idf=True, max_features=args.max_features)\n",
    "    \n",
    "    x_train_vec = vectorizer.fit_transform(x_train).toarray()\n",
    "    print_message('x_train_vec - type: {}, shape:{}'.format(type(x_train_vec),x_train_vec.shape),args)\n",
    "    \n",
    "    x_test_vec = vectorizer.fit_transform(x_test).toarray()\n",
    "    print_message('x_test_vec - type: {}, shape:{}'.format(type(x_test_vec),x_test_vec.shape),args)\n",
    "      \n",
    "    return x_train_vec, x_test_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion: load_dataset():         \n",
    "La misma función del archivo, donde llamamos a nuestra nueva rutina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.677576Z",
     "start_time": "2018-09-22T03:45:04.664435Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(args):\n",
    "    \n",
    "    print_new_process('Load and vectorize Data:',args)\n",
    "    \n",
    "    dataset = load_files('dataset/txt_sentoken', shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=42)\n",
    "    \n",
    "    print_message('Training samples {} ({}), test_samples {} ({})'.format(\n",
    "        len(X_train),\n",
    "        len(y_train), \n",
    "        len(X_test), \n",
    "        len(y_test)), args)\n",
    "\n",
    "    # TODO 1: Apply the Tfidf vectorizer to create input matrix\n",
    "    x_train_vec, x_test_vec = vectorize_input(X_train, X_test,args)\n",
    "        \n",
    "    return x_train_vec, x_test_vec, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Construir modelos Keras\n",
    "\n",
    "Se plantean modelos con diferentes arquitecturas para realizar las pruebas:\n",
    "\n",
    "### Build the Keras models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T04:07:56.486348Z",
     "start_time": "2018-09-22T04:07:56.470725Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "def build_keras_model_1(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense( args.num_units[0], input_shape=(input_size,)))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 1:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:48:21.412996Z",
     "start_time": "2018-09-22T03:48:21.315513Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_keras_model_2(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_size, input_dim=input_size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(int(input_size/2), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(int(input_size), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(int(input_size/2), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 2:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T04:09:04.415521Z",
     "start_time": "2018-09-22T04:09:04.388283Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_keras_model_3(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_size, args.num_units[0], input_shape=(input_size,)))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 3:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM, Flatten\n",
    "\n",
    "def build_keras_model_4(x_train_vec, args):\n",
    " \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense(args.num_units[0], input_shape=(input_size,)))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    model.add(Dense( int(input_size/2) ))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))       \n",
    "\n",
    "    model.add(Dense( int(input_size/4) ))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 4:', args )\n",
    "    print_message( model.summary(), args )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Main:\n",
    "\n",
    "Función principal que llama a las rutinas anteriores para realizar el entrenamiento. \n",
    "\n",
    "__Responde al punto:__\n",
    "\n",
    "_2. Construir un pipeline de clasificación con un modelo Keras MLP._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.854652Z",
     "start_time": "2018-09-22T03:45:04.743319Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    experiment_number = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    args = read_args()\n",
    "    \n",
    "    print_new_process('START:', args)\n",
    "    \n",
    "    # Configuramos la semilla aleatoria por reproducibilidad\n",
    "    np.random.seed(args.random_seed)\n",
    "    \n",
    "    # Cargamos el dataset\n",
    "    x_train_vec, x_test_vec, y_train, y_test_orginal = load_dataset(args)\n",
    "\n",
    "    # TODO 2: Convert the labels to categorical\n",
    "    num_classes = 2 \n",
    "    y_train_cat = np_utils.to_categorical(y_train, num_classes)    \n",
    "    y_test_cat = np_utils.to_categorical(y_test_orginal, num_classes)    \n",
    "    \n",
    "    print_new_process('Build Model:', args )\n",
    "    # TODO 3: Build the Keras model\n",
    "    switcher = {\n",
    "        1: build_keras_model_1,\n",
    "        2: build_keras_model_2,\n",
    "        3: build_keras_model_3,\n",
    "        4: build_keras_model_4,\n",
    "    }\n",
    "    # Get the function from switcher dictionary\n",
    "    model_builder = switcher.get(args.model, lambda: \"nothing\")\n",
    "    \n",
    "    model = model_builder(x_train_vec, args)\n",
    "    \n",
    "    # TODO 4: Fit the model    \n",
    "    print_new_process('Fit:', args)\n",
    "    history = model.fit(x_train_vec, y_train_cat,\n",
    "                        batch_size=args.batch_size,\n",
    "                        epochs=args.epochs,\n",
    "                        shuffle=args.shuffle,\n",
    "                        verbose=1,\n",
    "             )\n",
    "\n",
    "    # TODO 5: Evaluate the model, calculating the metrics.\n",
    "    # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "    # already compiled with the metrics.\n",
    "    print_new_process('Predictions:',args)\n",
    "    predictions = model.predict_classes(x_test_vec, verbose=1)\n",
    "    \n",
    "    if args.verbose == 1:\n",
    "        display(str(list( predictions)))\n",
    "\n",
    "    print_new_process('Test:',args)\n",
    "    if args.verbose == 1:        \n",
    "        display(str(list(y_test_orginal)))\n",
    "    \n",
    "    print_new_process('Performance:',args)\n",
    "    score, accuracy = model.evaluate(x_test_vec, y_test_cat)\n",
    "    print_message('[score, accuracy]', args)\n",
    "    print_message([score, accuracy], args)\n",
    "\n",
    "    # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "    # sklearn. We recommend this, because you can store the predictions if\n",
    "    # you need more analysis later. Also, if you calculate the metrics on a\n",
    "    # notebook, then you can compare multiple classifiers.\n",
    "    # predictions = ...\n",
    "    # performance = ...\n",
    "\n",
    "    # TODO 6: Save the results.    \n",
    "    parameters_path = 'results/parameters/'\n",
    "    if not os.path.exists(parameters_path):\n",
    "        os.makedirs(parameters_path)\n",
    "    predictions_path = 'results/predictions/'\n",
    "    if not os.path.exists(predictions_path):\n",
    "        os.makedirs(predictions_path)\n",
    "\n",
    "    parameters_filename = 'results/parameters/parameters_{0}_acc_{1:.6f}.csv'.format(experiment_number, accuracy)\n",
    "    print_new_process('Saving parameters: {}'.format(parameters_filename),args)  \n",
    "    \n",
    "    parameters_df = pandas.DataFrame(columns=['Parameter','Value'])\n",
    "    for k,v in sorted(vars(args).items()):\n",
    "        row = pandas.Series([str(k), str(v)], index=['Parameter', 'Value'])\n",
    "        parameters_df = parameters_df.append(row,ignore_index=True)\n",
    "    parameters_df.to_csv(parameters_filename, index=False )\n",
    "    \n",
    "    predictions_filename = 'results/predictions/predictions_{0}_acc_{1:.6f}.csv'.format(experiment_number, accuracy)\n",
    "    print_new_process('Saving predictions: {}'.format(predictions_filename),args)  \n",
    "    predictions_df = pandas.DataFrame(y_test_orginal, columns=['true_label'])\n",
    "    predictions_df.loc[:, 'predicted'] = predictions\n",
    "    predictions_df.to_csv(predictions_filename, index=False)\n",
    "    \n",
    "    print_message('Done.',args)    \n",
    "    print_end('STOP.',args)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Ejecución del modelo:\n",
    "\n",
    "__Responde al punto:__\n",
    "\n",
    "_3. Entrenar uno o varios modelos (con dos o tres es suficiente, veremos más de esto en el práctico 2). Evaluar los modelos en el conjunto de test._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "Ejecutamos una prueba de main para poder visualizar que el funcionamiento sea adecuado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T04:10:46.640400Z",
     "start_time": "2018-09-22T04:09:09.824815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-08 16:46:23 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:23 - START:\n",
      "2018-12-08 16:46:23 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:23 - Load and vectorize Data:\n",
      "2018-12-08 16:46:30 - Training samples 1500 (1500), test_samples 500 (500)\n",
      "2018-12-08 16:46:31 - x_train_vec - type: <class 'numpy.ndarray'>, shape:(1500, 6000)\n",
      "2018-12-08 16:46:31 - x_test_vec - type: <class 'numpy.ndarray'>, shape:(500, 6000)\n",
      "2018-12-08 16:46:31 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:31 - Build Model:\n",
      "2018-12-08 16:46:32 - MODEL 1:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 16)                96016     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 96,050\n",
      "Trainable params: 96,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2018-12-08 16:46:32 - None\n",
      "2018-12-08 16:46:32 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:32 - Fit:\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 0s 306us/step - loss: 0.6668 - acc: 0.6340\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 0s 78us/step - loss: 0.5662 - acc: 0.8767\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 0s 85us/step - loss: 0.4921 - acc: 0.9140\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 0s 94us/step - loss: 0.4324 - acc: 0.9267\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 0s 93us/step - loss: 0.3825 - acc: 0.9500\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 0s 102us/step - loss: 0.3467 - acc: 0.9513\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 0s 95us/step - loss: 0.3204 - acc: 0.9633\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 0s 92us/step - loss: 0.2925 - acc: 0.9640\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 0s 86us/step - loss: 0.2715 - acc: 0.9673\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 0s 97us/step - loss: 0.2539 - acc: 0.9727\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 0s 92us/step - loss: 0.2373 - acc: 0.9760\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 0s 124us/step - loss: 0.2216 - acc: 0.9813\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 0s 100us/step - loss: 0.2089 - acc: 0.9807\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 0s 107us/step - loss: 0.2001 - acc: 0.9840\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 0s 94us/step - loss: 0.1852 - acc: 0.9900\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 0s 110us/step - loss: 0.1771 - acc: 0.9907\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 0s 115us/step - loss: 0.1694 - acc: 0.9900\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 0s 112us/step - loss: 0.1579 - acc: 0.9920\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 0s 105us/step - loss: 0.1554 - acc: 0.9867\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 0s 94us/step - loss: 0.1427 - acc: 0.9927\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 0s 90us/step - loss: 0.1381 - acc: 0.9940\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 0s 87us/step - loss: 0.1332 - acc: 0.9960\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 0s 83us/step - loss: 0.1280 - acc: 0.9953\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 0s 135us/step - loss: 0.1226 - acc: 0.9940\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 0s 91us/step - loss: 0.1199 - acc: 0.9967\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 0s 113us/step - loss: 0.1124 - acc: 0.9947\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 0s 107us/step - loss: 0.1088 - acc: 0.9960\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 0s 87us/step - loss: 0.1059 - acc: 0.9967\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 0s 82us/step - loss: 0.1015 - acc: 0.9980\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 0s 87us/step - loss: 0.0968 - acc: 0.9960\n",
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:37 - Predictions:\n",
      "500/500 [==============================] - 0s 217us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:37 - Test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:37 - Performance:\n",
      "500/500 [==============================] - 0s 288us/step\n",
      "2018-12-08 16:46:37 - [score, accuracy]\n",
      "2018-12-08 16:46:37 - [0.8631495027542114, 0.5300000009536743]\n",
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:37 - Saving parameters: results/parameters/parameters_20181208164623_acc_0.530000.csv\n",
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:37 - Saving predictions: results/predictions/predictions_20181208164623_acc_0.530000.csv\n",
      "2018-12-08 16:46:37 - Done.\n",
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n",
      "2018-12-08 16:46:37 - STOP.\n",
      "2018-12-08 16:46:37 ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cargamos argumentos de prueba\n",
    "arguments = ['--model=1',\n",
    "             '--max_features=6000',\n",
    "             '--num_units=16',\n",
    "             '--dropout=0.2',\n",
    "             '--batch_size=200',             \n",
    "             '--epochs=30',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\n",
    "# Ejecutamos el procedimiento principal\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reporte \n",
    "\n",
    "__Responde al punto:__\n",
    "_Reportar los hyperparámetros y resultados de todos los modelos entrenados. Para esto, pueden utilizar una notebook o un archivo (pdf|md). Dentro de este reporte tiene que describir:_\n",
    "\n",
    "* _Hyperparámetros con los que procesaron el dataset: tamaño del vocabulario, normalizaciones, etc._\n",
    "* _Las decisiones tomadas al construir cada modelo: regularización, dropout, número y tamaño de las capas, optimizador._\n",
    "* _Proceso de entrenamiento: división del train/test, tamaño del batch, número de épocas, métricas de evaluación._ \n",
    "* _Seleccione los mejores hiperparámetros en función de su rendimiento. El proceso de entrenamiento debería ser el mismo\n",
    "para todos los modelos._\n",
    "* _(Punto estrella) Analizar si el clasificador está haciendo overfitting. Esto se puede determinar a partir del resultado del método fit._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Reporte de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Descripción de los Modelos Utilizados\n",
    "\n",
    "Se describen a continuación los modelos utilizados para el trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Descripción del Modelo 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Descripción del Modelo 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Descripción del Modelo 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Descripción del Modelo 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Descripción de Proceso de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Selección de los mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
