{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis del archivo exercise_1.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primera parte del archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/HDDatos1/anaconda/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "import argparse\n",
    "import pandas\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.utils import np_utils   # for tf 1.3.1\n",
    "#from tensorflow.python.keras import utils as np_utils     # for tf 1.4.1\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import repeat\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from printutils import print_message, print_new_process, print_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_args():\n",
    "    parser = argparse.ArgumentParser(description='Exercise 1')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "    parser.add_argument('--num_units', nargs='+', default=[100], type=int,\n",
    "                        help='Number of hidden units of each hidden layer.')\n",
    "    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "                        help='Dropout ratio for every layer.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Number of instances in each batch.')\n",
    "\n",
    "    # New parameters:    \n",
    "    parser.add_argument('--model', type=int, default=10, help='Number of model to run')\n",
    "    parser.add_argument('--max_features', type=int, default=2000, help='Max number of words used inTfidfVectorizer')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')\n",
    "    parser.add_argument('--shuffle', type=str, default='batch', help='Shuffle value')\n",
    "    parser.add_argument('--random_seed', type=int, default=10, help='Random seed number')\n",
    "    parser.add_argument('--verbose', type=int, default=1, help='Verbose info on screen')\n",
    "    \n",
    "    # parse parameters\n",
    "    if arguments == None:\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        args = parser.parse_args(arguments)\n",
    "\n",
    "    assert len(args.num_units) == len(args.dropout)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the Tfidf vectorizer to create input matrix\n",
    "Creamos un método para vectorizar la entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "# TODO 1: Apply the Tfidf vectorizer to create input matrix\n",
    "def vectorize_input(x_train, x_test,args):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', use_idf=True, max_features=args.max_features)\n",
    "    \n",
    "    x_train_vec = vectorizer.fit_transform(x_train).toarray()\n",
    "    print_message('x_train_vec - type: {}, shape:{}'.format(type(x_train_vec),x_train_vec.shape),args)\n",
    "    \n",
    "    x_test_vec = vectorizer.fit_transform(x_test).toarray()\n",
    "    print_message('x_test_vec - type: {}, shape:{}'.format(type(x_test_vec),x_test_vec.shape),args)\n",
    "      \n",
    "    return x_train_vec, x_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion: load_dataset():         \n",
    "La misma función del archivo, donde llamamos a nuestra nueva rutina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(args):\n",
    "    \n",
    "    print_new_process('Load and vectorize Data:',args)\n",
    "    \n",
    "    dataset = load_files('dataset/txt_sentoken', shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=42)\n",
    "    \n",
    "    print_message('Training samples {}, test_samples {}'.format(len(X_train), len(X_test)), args)\n",
    "\n",
    "    # TODO 1: Apply the Tfidf vectorizer to create input matrix\n",
    "    x_train_vec, x_test_vec = vectorize_input(X_train, X_test,args)\n",
    "        \n",
    "    return x_train_vec, x_test_vec, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Keras models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "def build_keras_model_1( x_train_vec, args ):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense( args.num_units[0], input_shape=(input_size,)))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 1:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_keras_model_2( x_train_vec, args ):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 2:', args )\n",
    "    print_message( model.summary() )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_keras_model_3( x_train_vec, args ):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 3:', args )\n",
    "    print_message( model.summary() )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos main para llamar a las rutinas anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    experiment_number = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    args = read_args()\n",
    "    \n",
    "    print_new_process('START:', args )\n",
    "    \n",
    "    # Configuramos la semilla randomica para reproductibilidad\n",
    "    np.random.seed(args.random_seed)\n",
    "    \n",
    "    # Cargamos el dataset\n",
    "    x_train_vec, x_test_vec, y_train, y_test_orginal = load_dataset(args)\n",
    "\n",
    "    # TODO 2: Convert the labels to categorical\n",
    "    num_classes = 2 \n",
    "    y_train_cat = np_utils.to_categorical(y_train, num_classes)    \n",
    "    y_test_cat = np_utils.to_categorical(y_test_orginal, num_classes)    \n",
    "    \n",
    "    print_new_process('Build Model:', args )\n",
    "    # TODO 3: Build the Keras model\n",
    "    switcher = {\n",
    "        1: build_keras_model_1,\n",
    "        2: build_keras_model_2,\n",
    "        3: build_keras_model_3,\n",
    "    }\n",
    "    # Get the function from switcher dictionary\n",
    "    model_builder = switcher.get(args.model, lambda: \"nothing\")\n",
    "    \n",
    "    model = model_builder( x_train_vec, args )\n",
    "    \n",
    "    # TODO 4: Fit the model    \n",
    "    print_new_process('Fit:', args)\n",
    "    history = model.fit( x_train_vec, y_train_cat,\n",
    "                        batch_size=args.batch_size,\n",
    "                        epochs=args.epochs,\n",
    "                        shuffle=args.shuffle,\n",
    "                        verbose=1,\n",
    "             )\n",
    "\n",
    "    # TODO 5: Evaluate the model, calculating the metrics.\n",
    "    # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "    # already compiled with the metrics.\n",
    "    \n",
    "    print_new_process('Predictions:',args)\n",
    "    predictions = model.predict_classes(x_test_vec, verbose=1)\n",
    "    \n",
    "    if args.verbose == 1:\n",
    "        print( predictions )\n",
    "\n",
    "    print_new_process('Test:',args)\n",
    "    if args.verbose == 1:        \n",
    "        print( np.reshape(y_test_cat, -1).astype(int) )\n",
    "    \n",
    "    print_new_process('Performance:',args)\n",
    "    score, accuracy = model.evaluate(x_test_vec, y_test_cat)\n",
    "    print_message( '[score, accuracy]', args )\n",
    "    print_message( [score, accuracy], args )\n",
    "\n",
    "    # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "    # sklearn. We recommend this, because you can store the predictions if\n",
    "    # you need more analysis later. Also, if you calculate the metrics on a\n",
    "    # notebook, then you can compare multiple classifiers.\n",
    "    # predictions = ...\n",
    "    # performance = ...\n",
    "\n",
    "    # TODO 6: Save the results.    \n",
    "    parameters_path = 'results/parameters/'\n",
    "    if not os.path.exists(parameters_path):\n",
    "        os.makedirs(parameters_path)\n",
    "    predictions_path = 'results/predictions/'\n",
    "    if not os.path.exists(predictions_path):\n",
    "        os.makedirs(predictions_path)\n",
    "\n",
    "    parameters_filename = 'results/parameters/parameters_{0}_acc_{1:.6f}.csv'.format( experiment_number, accuracy )\n",
    "    print_new_process('Saving parameters: {}'.format(parameters_filename),args)  \n",
    "    \n",
    "    parameters_df = pandas.DataFrame(columns=['Parameter','Value'])\n",
    "    for k,v in sorted(vars(args).items()):\n",
    "        row = pandas.Series([str(k), str(v)], index=['Parameter', 'Value'])\n",
    "        parameters_df = parameters_df.append(row,ignore_index=True)\n",
    "    parameters_df.to_csv(parameters_filename, index=False )\n",
    "    \n",
    "    predictions_filename = 'results/predictions/predictions_{0}_acc_{1:.6f}.csv'.format( experiment_number, accuracy )\n",
    "    print_new_process('Saving predictions: {}'.format(predictions_filename),args)  \n",
    "    predictions_df = pandas.DataFrame(y_test_orginal, columns=['true_label'])\n",
    "    predictions_df.loc[:, 'predicted'] = predictions\n",
    "    predictions_df.to_csv( predictions_filename, index=False )\n",
    "    \n",
    "    print_message('Done.',args)    \n",
    "    print_end('STOP.',args)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de Main:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "Ejecutamos una prueba de main para poder visualizar que el funcionamiento sea adecuado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-21 07:29:30 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:30 - START:\n",
      "2018-09-21 07:29:30 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:30 - Load and vectorize Data:\n",
      "2018-09-21 07:29:31 - Training samples 1500, test_samples 500\n",
      "2018-09-21 07:29:34 - x_train_vec - type: <class 'numpy.ndarray'>, shape:(1500, 2000)\n",
      "2018-09-21 07:29:35 - x_test_vec - type: <class 'numpy.ndarray'>, shape:(500, 2000)\n",
      "2018-09-21 07:29:35 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:35 - Build Model:\n",
      "2018-09-21 07:29:35 - MODEL 1:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               1024512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,025,538\n",
      "Trainable params: 1,025,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2018-09-21 07:29:35 - None\n",
      "2018-09-21 07:29:35 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:35 - Fit:\n",
      "Epoch 1/15\n",
      "1500/1500 [==============================] - 1s 626us/step - loss: 0.6118 - acc: 0.6793\n",
      "Epoch 2/15\n",
      "1500/1500 [==============================] - 0s 289us/step - loss: 0.3957 - acc: 0.8527\n",
      "Epoch 3/15\n",
      "1500/1500 [==============================] - 0s 276us/step - loss: 0.2647 - acc: 0.9300\n",
      "Epoch 4/15\n",
      "1500/1500 [==============================] - 0s 288us/step - loss: 0.1943 - acc: 0.9520\n",
      "Epoch 5/15\n",
      "1500/1500 [==============================] - 0s 287us/step - loss: 0.1543 - acc: 0.9673\n",
      "Epoch 6/15\n",
      "1500/1500 [==============================] - 0s 296us/step - loss: 0.1207 - acc: 0.9833\n",
      "Epoch 7/15\n",
      "1500/1500 [==============================] - 0s 285us/step - loss: 0.1009 - acc: 0.9820\n",
      "Epoch 8/15\n",
      "1500/1500 [==============================] - 0s 289us/step - loss: 0.0843 - acc: 0.9913\n",
      "Epoch 9/15\n",
      "1500/1500 [==============================] - 0s 285us/step - loss: 0.0680 - acc: 0.9960\n",
      "Epoch 10/15\n",
      "1500/1500 [==============================] - 0s 294us/step - loss: 0.0568 - acc: 0.9980\n",
      "Epoch 11/15\n",
      "1500/1500 [==============================] - 0s 297us/step - loss: 0.0497 - acc: 0.9973\n",
      "Epoch 12/15\n",
      "1500/1500 [==============================] - 0s 279us/step - loss: 0.0419 - acc: 0.9993\n",
      "Epoch 13/15\n",
      "1500/1500 [==============================] - 0s 292us/step - loss: 0.0384 - acc: 0.9993\n",
      "Epoch 14/15\n",
      "1500/1500 [==============================] - 0s 274us/step - loss: 0.0328 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "1500/1500 [==============================] - 0s 278us/step - loss: 0.0298 - acc: 1.0000\n",
      "2018-09-21 07:29:43 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:43 - Predictions:\n",
      "500/500 [==============================] - 0s 210us/step\n",
      "[0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
      " 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
      " 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1\n",
      " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0\n",
      " 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0\n",
      " 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1]\n",
      "2018-09-21 07:29:43 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:43 - Test:\n",
      "[0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
      " 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
      " 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
      " 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1\n",
      " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1\n",
      " 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1\n",
      " 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0\n",
      " 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1\n",
      " 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1\n",
      " 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
      " 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
      " 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1\n",
      " 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1\n",
      " 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0\n",
      " 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
      " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1\n",
      " 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1\n",
      " 0]\n",
      "2018-09-21 07:29:43 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:43 - Performance:\n",
      "500/500 [==============================] - 0s 214us/step\n",
      "2018-09-21 07:29:43 - [score, accuracy]\n",
      "2018-09-21 07:29:43 - [1.4899961042404175, 0.46200000047683715]\n",
      "2018-09-21 07:29:43 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:43 - Saving parameters: results/parameters/parameters_20180921072930_acc_0.462000.csv\n",
      "2018-09-21 07:29:43 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:43 - Saving predictions: results/predictions/predictions_20180921072930_acc_0.462000.csv\n",
      "2018-09-21 07:29:43 - Done.\n",
      "2018-09-21 07:29:43 ------------------------------------------------------------------------\n",
      "2018-09-21 07:29:44 - STOP.\n",
      "2018-09-21 07:29:44 ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# cargamos argumentos de prueba\n",
    "arguments = ['--model=1',\n",
    "             '--max_features=2000',\n",
    "             '--num_units=512',\n",
    "             '--dropout=0.5',\n",
    "             '--batch_size=100',             \n",
    "             '--epochs=15',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\n",
    "# Ejecutamos el procedimiento principal\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
