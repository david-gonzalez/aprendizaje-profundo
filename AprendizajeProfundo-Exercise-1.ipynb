{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universidad Nacional de Córdoba\n",
    "# DIPLODATOS - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "## Trabajo Práctico Número 1 (Analisis del archivo exercise_1.py)\n",
    "\n",
    "Autores: \n",
    "* Diaz Cobos Facundo\n",
    "* Epifanio Luis\n",
    "* Gonzalez Leonardo David\n",
    "* Gualpa Mariano Martín\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Consigna del Ejercicio 1.\n",
    "\n",
    "1. Procesar el conjunto de datos para obtener una representación TfIDf de cada review.\n",
    "  * Hint: User el código de *Aprendizaje Supervisado*\n",
    "  * Hint: El método de fit de Keras crea internamente un conjunto de datos de validación, por lo que no tienen que preocuparse por eso.\n",
    "\n",
    "2. Construir un pipeline de clasificación con un modelo Keras MLP.\n",
    "\n",
    "3. Entrenar uno o varios modelos (con dos o tres es suficiente, veremos más de esto en el práctico 2). Evaluar los modelos en el conjunto de test.\n",
    "\n",
    "4. Reportar los hyperparámetros y resultados de todos los modelos entrenados. Para esto, pueden utilizar una notebook o un archivo (pdf|md). Dentro de este reporte tiene que describir:\n",
    "  * Hyperparámetros con los que procesaron el dataset: tamaño del vocabulario, normalizaciones, etc.\n",
    "  * Las decisiones tomadas al construir cada modelo: regularización, dropout, número y tamaño de las capas, optimizador.\n",
    "  * Proceso de entrenamiento: división del train/test, tamaño del batch, número de épocas, métricas de evaluación. Seleccione los mejores hiperparámetros en función de su rendimiento. El proceso de entrenamiento debería ser el mismo para todos los modelos.\n",
    "  * (Punto estrella) Analizar si el clasificador está haciendo overfitting. Esto se puede determinar a partir del resultado del método fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Desarrollo.\n",
    "\n",
    "## 2.1. Carga de Librerías y Funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:02.949218Z",
     "start_time": "2018-09-22T03:45:02.936300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it work on Colab.\n",
    "#!wget --continue https://github.com/david-gonzalez/aprendizaje-profundo/raw/master/dataset/txt_sentoken.zip\n",
    "#!mkdir dataset && mv txt_sentoken.zip ./dataset\n",
    "#!cd dataset && unzip txt_sentoken.zip\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.613427Z",
     "start_time": "2018-09-22T03:45:02.951091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from itertools import repeat\n",
    "\n",
    "from keras import regularizers, layers, optimizers, losses, metrics\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from printutils import print_message, print_new_process, print_end\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.python.keras.utils import np_utils   # for tf 1.3.1\n",
    "from tensorflow.test import gpu_device_name\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.637078Z",
     "start_time": "2018-09-22T03:45:04.616162Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_args():\n",
    "    parser = argparse.ArgumentParser(description='Exercise 1')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "    parser.add_argument('--num_units', nargs='+', default=[100], type=int,\n",
    "                        help='Number of hidden units of each hidden layer.')\n",
    "    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "                        help='Dropout ratio for every layer.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Number of instances in each batch.')\n",
    "\n",
    "    # New parameters:    \n",
    "    parser.add_argument('--model', type=int, default=10, help='Number of model to run')\n",
    "    parser.add_argument('--max_features', type=int, default=2000, help='Max number of words used inTfidfVectorizer')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')\n",
    "    parser.add_argument('--shuffle', type=str, default='batch', help='Shuffle value')\n",
    "    parser.add_argument('--random_seed', type=int, default=10, help='Random seed number')\n",
    "    parser.add_argument('--verbose', type=int, default=1, help='Verbose info on screen')\n",
    "    \n",
    "    # parse parameters\n",
    "    if arguments == None:\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        args = parser.parse_args(arguments)\n",
    "\n",
    "    assert len(args.num_units) == len(args.dropout)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Procesar el conjunto de datos utilizando TfIDf vectorizer para crear una matriz de entrada.\n",
    "\n",
    "__Responde al punto:__\n",
    "\n",
    "_1. Procesar el conjunto de datos para obtener una representación TfIDf de cada review._\n",
    "  * _Hint: User el código de *Aprendizaje Supervisado*_\n",
    "  * _Hint: El método de fit de Keras crea internamente un conjunto de datos de validación, por lo que no tienen que preocuparse por eso._\n",
    "\n",
    "\n",
    "### Apply the Tfidf vectorizer to create input matrix\n",
    "Creamos un método para vectorizar la entrada, pero antes aplicamos algo de data cleaning para mejorar la snesitividad del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variable\n",
    "LANGUAGE='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list()     \n",
    "nltk_words = stopwords.words(LANGUAGE)\n",
    "\n",
    "custom_words = []\n",
    "\n",
    "stop_words.extend(nltk_words)\n",
    "stop_words.extend(custom_words)\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load an Stemmer\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(LANGUAGE)\n",
    "#\n",
    "'''\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def text_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "'''\n",
    "\n",
    "def tokenize_and_stem(text):    \n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    tokens = [\n",
    "        word #.translate(translator)\n",
    "        for sent in sent_tokenize( text ) \n",
    "        for word in word_tokenize( sent )\n",
    "    ]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    #stems = [t for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.661331Z",
     "start_time": "2018-09-22T03:45:04.639663Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1: Apply the Tfidf vectorizer to create input matrix\n",
    "def vectorize_input(x_train, x_test,args):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word', \n",
    "        max_df        = 0.97, \n",
    "        max_features  = None, #args.max_features,\n",
    "        min_df        = 0.01,\n",
    "        #ngram_range   = (1,3),\n",
    "        norm          = 'l2',\n",
    "        stop_words    = stop_words,\n",
    "        strip_accents = 'ascii',\n",
    "        tokenizer     = tokenize_and_stem,\n",
    "        use_idf       = True \n",
    "    )\n",
    "    \n",
    "    \n",
    "    x_train_vec = vectorizer.fit_transform(x_train).toarray()\n",
    "    x_test_vec = vectorizer.transform(x_test).toarray()\n",
    "    \n",
    "    norm = Normalizer().fit(x_train_vec)\n",
    "    \n",
    "    x_train_vec = norm.transform(x_train_vec)\n",
    "    x_test_vec  = norm.transform(x_test_vec)\n",
    "    \n",
    "    print_message('x_train_vec - type: {}, shape:{}'.format(type(x_train_vec),x_train_vec.shape),args)\n",
    "    print_message('x_test_vec - type: {}, shape:{}'.format(type(x_test_vec),x_test_vec.shape),args)\n",
    "    \n",
    "    print('total detected features {}'.format( \n",
    "            len( vectorizer.get_feature_names() ) \n",
    "            ) \n",
    "     )\n",
    "    print('vectorizer {}'.format( vectorizer ))\n",
    "      \n",
    "    return x_train_vec, x_test_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion: load_dataset():         \n",
    "La misma función del archivo, donde llamamos a nuestra nueva rutina:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.677576Z",
     "start_time": "2018-09-22T03:45:04.664435Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "def load_dataset(args):\n",
    "    \n",
    "    print_new_process('Load and vectorize Data:',args)\n",
    "    \n",
    "    dataset = load_files('dataset/txt_sentoken', shuffle=False)\n",
    "\n",
    "    #(X_train, X_test), (y_train, y_test) = imdb.load_data()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=10042)\n",
    "    \n",
    "    print_message('Training samples {} ({}), test_samples {} ({})'.format(\n",
    "        len(X_train),\n",
    "        len(y_train), \n",
    "        len(X_test), \n",
    "        len(y_test)), args)\n",
    "\n",
    "    # TODO 1: Apply the Tfidf vectorizer to create input matrix\n",
    "    x_train_vec, x_test_vec = vectorize_input(X_train, X_test,args)\n",
    "        \n",
    "    return x_train_vec, x_test_vec, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Construir modelos Keras\n",
    "\n",
    "Se plantean modelos con diferentes arquitecturas para realizar las pruebas:\n",
    "\n",
    "### Build the Keras models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T04:07:56.486348Z",
     "start_time": "2018-09-22T04:07:56.470725Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "def build_keras_model_1(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense( args.num_units[0], input_shape=(input_size,)))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 1:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    \"\"\"\n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense( args.num_units[0], input_shape=(input_size,)))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 1:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:48:21.412996Z",
     "start_time": "2018-09-22T03:48:21.315513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndef build_keras_model_2(x_train_vec, args):\\n    \\n    input_size = x_train_vec.shape[1]\\n       \\n    model = Sequential()\\n\\n    model.add(Dense(input_size, input_dim=input_size, kernel_initializer='normal', activation='relu'))\\n    model.add(Dropout(args.dropout[0]))\\n    model.add(Dense(int(input_size/2), kernel_initializer='normal', activation='relu'))\\n    model.add(Dropout(args.dropout[0]))\\n    model.add(Dense(int(input_size), kernel_initializer='normal', activation='relu'))\\n    model.add(Dropout(args.dropout[0]))\\n    model.add(Dense(int(input_size/2), kernel_initializer='normal', activation='relu'))\\n    model.add(Dropout(args.dropout[0]))\\n    model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))\\n    \\n    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\\n    \\n    # Show model info on screen\\n    print_message('MODEL 2:', args )\\n    print_message( model.summary(), args )\\n\\n    return model\\n    \\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_keras_model_2(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_size, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(input_size,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, kernel_regularizer=regularizers.l1(0.001),activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Use of REGULARIZATION\n",
    "    #model = models.Sequential()\n",
    "    #model.add(Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu', input_shape=(10000,)))\n",
    "    #model.add(Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu'))\n",
    "    #model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # REGULARIZERS L1 L2\n",
    "    #regularizers.l1(0.001)\n",
    "    #regularizers.l2(0.001)\n",
    "    #regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 2:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def build_keras_model_2(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_size, input_dim=input_size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(int(input_size/2), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(int(input_size), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(int(input_size/2), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 2:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T04:09:04.415521Z",
     "start_time": "2018-09-22T04:09:04.388283Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_keras_model_3(x_train_vec, args):\n",
    "    \n",
    "    input_size = x_train_vec.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_size, args.num_units[0], input_shape=(input_size,)))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 3:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM, Flatten\n",
    "\n",
    "def build_keras_model_4(x_train_vec, args):\n",
    " \n",
    "    input_size = x_train_vec.shape[1]\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense(args.num_units[0], input_shape=(input_size,)))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    model.add(Dense( int(input_size/2) ))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))       \n",
    "\n",
    "    model.add(Dense( int(input_size/4) ))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL 4:', args )\n",
    "    print_message( model.summary(), args )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Main:\n",
    "\n",
    "Función principal que llama a las rutinas anteriores para realizar el entrenamiento. \n",
    "\n",
    "__Responde al punto:__\n",
    "\n",
    "_2. Construir un pipeline de clasificación con un modelo Keras MLP._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T03:45:04.854652Z",
     "start_time": "2018-09-22T03:45:04.743319Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def main():\n",
    "\n",
    "    USE_CATEGORICAL = True\n",
    "    \n",
    "    experiment_number = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    args = read_args()\n",
    "    \n",
    "    print_new_process('START:', args)\n",
    "    \n",
    "    # Configuramos la semilla aleatoria por reproducibilidad\n",
    "    np.random.seed(args.random_seed)\n",
    "    \n",
    "    # Cargamos el dataset\n",
    "    x_train_vec, x_test_vec, y_train, y_test_orginal = load_dataset(args)\n",
    "\n",
    "    # TODO 2: Convert the labels to categorical\n",
    "    if (USE_CATEGORICAL):\n",
    "        num_classes = 2 \n",
    "        #y_train_cat = np_utils.to_categorical(y_train, num_classes)    \n",
    "        #y_test_cat = np_utils.to_categorical(y_test_orginal, num_classes)    \n",
    "        y_train_cat = to_categorical(y_train)    \n",
    "        y_test_cat = to_categorical(y_test_orginal)    \n",
    "\n",
    "    else:\n",
    "        y_train_cat = y_train    \n",
    "        y_test_cat = y_test_orginal    \n",
    "\n",
    "    \n",
    "    \n",
    "    print_new_process('Build Model:', args )\n",
    "    # TODO 3: Build the Keras model\n",
    "    switcher = {\n",
    "        1: build_keras_model_1,\n",
    "        2: build_keras_model_2,\n",
    "        3: build_keras_model_3,\n",
    "        4: build_keras_model_4,\n",
    "    }\n",
    "    # Get the function from switcher dictionary\n",
    "    model_builder = switcher.get(args.model, lambda: \"nothing\")\n",
    "    \n",
    "    model = model_builder(x_train_vec, args)\n",
    "    \n",
    "    # TODO 4: Fit the model    \n",
    "    print_new_process('Fit:', args)\n",
    "    history = model.fit(x_train_vec, y_train_cat,\n",
    "                        batch_size=args.batch_size,\n",
    "                        epochs=args.epochs,\n",
    "                        shuffle=args.shuffle,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.70\n",
    "             )\n",
    "\n",
    "    # TODO 5: Evaluate the model, calculating the metrics.\n",
    "    # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "    # already compiled with the metrics.\n",
    "    print_new_process('Predictions:',args)\n",
    "    predictions = history.model.predict_classes(x_test_vec, verbose=1)\n",
    "    \n",
    "    if args.verbose == 1:\n",
    "        display(str(list( predictions)))\n",
    "\n",
    "    print_new_process('Test:',args)\n",
    "    if args.verbose == 1:        \n",
    "        display(str(list(y_test_orginal)))\n",
    "    \n",
    "    print_new_process('Performance:',args)\n",
    "    score, accuracy = history.model.evaluate(x_test_vec, y_test_cat)\n",
    "    #score, accuracy = model.evaluate(x_train_vec, y_train_cat)\n",
    "\n",
    "    print_message('[score, accuracy]', args)\n",
    "    print_message([score, accuracy], args)\n",
    "\n",
    "    # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "    # sklearn. We recommend this, because you can store the predictions if\n",
    "    # you need more analysis later. Also, if you calculate the metrics on a\n",
    "    # notebook, then you can compare multiple classifiers.\n",
    "    # predictions = ...\n",
    "    # performance = ...\n",
    "\n",
    "    # TODO 6: Save the results.    \n",
    "    parameters_path = 'results/parameters/'\n",
    "    if not os.path.exists(parameters_path):\n",
    "        os.makedirs(parameters_path)\n",
    "    predictions_path = 'results/predictions/'\n",
    "    if not os.path.exists(predictions_path):\n",
    "        os.makedirs(predictions_path)\n",
    "\n",
    "    parameters_filename = 'results/parameters/parameters_{0}_acc_{1:.6f}.csv'.format(experiment_number, accuracy)\n",
    "    print_new_process('Saving parameters: {}'.format(parameters_filename),args)  \n",
    "    \n",
    "    parameters_df = pd.DataFrame(columns=['Parameter','Value'])\n",
    "    for k,v in sorted(vars(args).items()):\n",
    "        row = pd.Series([str(k), str(v)], index=['Parameter', 'Value'])\n",
    "        parameters_df = parameters_df.append(row,ignore_index=True)\n",
    "    parameters_df.to_csv(parameters_filename, index=False )\n",
    "    \n",
    "    predictions_filename = 'results/predictions/predictions_{0}_acc_{1:.6f}.csv'.format(experiment_number, accuracy)\n",
    "    print_new_process('Saving predictions: {}'.format(predictions_filename),args)  \n",
    "    predictions_df = pd.DataFrame(y_test_orginal, columns=['true_label'])\n",
    "    predictions_df.loc[:, 'predicted'] = predictions\n",
    "    predictions_df.to_csv(predictions_filename, index=False)\n",
    "    \n",
    "    print_message('Done.',args)    \n",
    "    print_end('STOP.',args)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Ejecución del modelo:\n",
    "\n",
    "__Responde al punto:__\n",
    "\n",
    "_3. Entrenar uno o varios modelos (con dos o tres es suficiente, veremos más de esto en el práctico 2). Evaluar los modelos en el conjunto de test._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "Ejecutamos una prueba de main para poder visualizar que el funcionamiento sea adecuado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 22:57:19 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:19 - START:\n",
      "2018-12-10 22:57:19 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:19 - Load and vectorize Data:\n",
      "2018-12-10 22:57:19 - Training samples 1500 (1500), test_samples 500 (500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lepifanio/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'veri', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 22:57:43 - x_train_vec - type: <class 'numpy.ndarray'>, shape:(1500, 3877)\n",
      "2018-12-10 22:57:43 - x_test_vec - type: <class 'numpy.ndarray'>, shape:(500, 3877)\n",
      "total detected features 3877\n",
      "vectorizer TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.97, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words={'only', 'her', \"won't\", 'did', 'has', 'myself', 'am', 'most', 'me', 'aren', 'they', 'your', 'but', \"hadn't\", \"it's\", 'be', 'herself', 'those', 'after', 'below', 'himself', 'yours', 'once', \"shouldn't\", \"wouldn't\", 'been', 'them', \"you'd\", 'this', \"don't\", 'y', 'shouldn', 'until', 'all', ...isn', 're', 'needn', \"shan't\", 'too', 'off', 'while', 'are', 'at', 'who', 'yourself', 'm', 'during'},\n",
      "        strip_accents='ascii', sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize_and_stem at 0x1a349591e0>,\n",
      "        use_idf=True, vocabulary=None)\n",
      "2018-12-10 22:57:43 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:43 - Build Model:\n",
      "2018-12-10 22:57:43 - MODEL 1:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 16)                62048     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 62,082\n",
      "Trainable params: 62,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2018-12-10 22:57:43 - None\n",
      "2018-12-10 22:57:43 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:43 - Fit:\n",
      "Train on 450 samples, validate on 1050 samples\n",
      "Epoch 1/30\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.6905 - acc: 0.5044 - val_loss: 0.6724 - val_acc: 0.7124\n",
      "Epoch 2/30\n",
      "450/450 [==============================] - 0s 88us/step - loss: 0.6165 - acc: 0.9244 - val_loss: 0.6398 - val_acc: 0.7314\n",
      "Epoch 3/30\n",
      "450/450 [==============================] - 0s 118us/step - loss: 0.5450 - acc: 0.9267 - val_loss: 0.6176 - val_acc: 0.7638\n",
      "Epoch 4/30\n",
      "450/450 [==============================] - 0s 99us/step - loss: 0.4925 - acc: 0.9467 - val_loss: 0.6021 - val_acc: 0.7600\n",
      "Epoch 5/30\n",
      "450/450 [==============================] - 0s 102us/step - loss: 0.4441 - acc: 0.9489 - val_loss: 0.5884 - val_acc: 0.7686\n",
      "Epoch 6/30\n",
      "450/450 [==============================] - 0s 105us/step - loss: 0.4031 - acc: 0.9622 - val_loss: 0.5771 - val_acc: 0.7648\n",
      "Epoch 7/30\n",
      "450/450 [==============================] - 0s 92us/step - loss: 0.3712 - acc: 0.9756 - val_loss: 0.5673 - val_acc: 0.7676\n",
      "Epoch 8/30\n",
      "450/450 [==============================] - 0s 106us/step - loss: 0.3527 - acc: 0.9733 - val_loss: 0.5591 - val_acc: 0.7657\n",
      "Epoch 9/30\n",
      "450/450 [==============================] - 0s 91us/step - loss: 0.3312 - acc: 0.9756 - val_loss: 0.5523 - val_acc: 0.7667\n",
      "Epoch 10/30\n",
      "450/450 [==============================] - 0s 104us/step - loss: 0.3057 - acc: 0.9756 - val_loss: 0.5456 - val_acc: 0.7667\n",
      "Epoch 11/30\n",
      "450/450 [==============================] - 0s 90us/step - loss: 0.2914 - acc: 0.9822 - val_loss: 0.5401 - val_acc: 0.7676\n",
      "Epoch 12/30\n",
      "450/450 [==============================] - 0s 90us/step - loss: 0.2741 - acc: 0.9867 - val_loss: 0.5352 - val_acc: 0.7667\n",
      "Epoch 13/30\n",
      "450/450 [==============================] - 0s 100us/step - loss: 0.2544 - acc: 0.9911 - val_loss: 0.5306 - val_acc: 0.7657\n",
      "Epoch 14/30\n",
      "450/450 [==============================] - 0s 92us/step - loss: 0.2434 - acc: 0.9911 - val_loss: 0.5264 - val_acc: 0.7629\n",
      "Epoch 15/30\n",
      "450/450 [==============================] - 0s 109us/step - loss: 0.2311 - acc: 0.9933 - val_loss: 0.5228 - val_acc: 0.7629\n",
      "Epoch 16/30\n",
      "450/450 [==============================] - 0s 100us/step - loss: 0.2170 - acc: 0.9956 - val_loss: 0.5193 - val_acc: 0.7638\n",
      "Epoch 17/30\n",
      "450/450 [==============================] - 0s 105us/step - loss: 0.2106 - acc: 0.9867 - val_loss: 0.5160 - val_acc: 0.7629\n",
      "Epoch 18/30\n",
      "450/450 [==============================] - 0s 90us/step - loss: 0.2069 - acc: 0.9956 - val_loss: 0.5137 - val_acc: 0.7629\n",
      "Epoch 19/30\n",
      "450/450 [==============================] - 0s 102us/step - loss: 0.1858 - acc: 0.9978 - val_loss: 0.5108 - val_acc: 0.7638\n",
      "Epoch 20/30\n",
      "450/450 [==============================] - 0s 105us/step - loss: 0.1897 - acc: 0.9978 - val_loss: 0.5088 - val_acc: 0.7629\n",
      "Epoch 21/30\n",
      "450/450 [==============================] - 0s 95us/step - loss: 0.1822 - acc: 0.9956 - val_loss: 0.5064 - val_acc: 0.7619\n",
      "Epoch 22/30\n",
      "450/450 [==============================] - 0s 99us/step - loss: 0.1748 - acc: 0.9978 - val_loss: 0.5043 - val_acc: 0.7619\n",
      "Epoch 23/30\n",
      "450/450 [==============================] - 0s 102us/step - loss: 0.1651 - acc: 0.9978 - val_loss: 0.5025 - val_acc: 0.7600\n",
      "Epoch 24/30\n",
      "450/450 [==============================] - 0s 96us/step - loss: 0.1556 - acc: 0.9978 - val_loss: 0.5012 - val_acc: 0.7590\n",
      "Epoch 25/30\n",
      "450/450 [==============================] - 0s 108us/step - loss: 0.1531 - acc: 1.0000 - val_loss: 0.4995 - val_acc: 0.7571\n",
      "Epoch 26/30\n",
      "450/450 [==============================] - 0s 100us/step - loss: 0.1428 - acc: 1.0000 - val_loss: 0.4980 - val_acc: 0.7581\n",
      "Epoch 27/30\n",
      "450/450 [==============================] - 0s 103us/step - loss: 0.1428 - acc: 0.9978 - val_loss: 0.4969 - val_acc: 0.7629\n",
      "Epoch 28/30\n",
      "450/450 [==============================] - 0s 102us/step - loss: 0.1390 - acc: 0.9956 - val_loss: 0.4956 - val_acc: 0.7610\n",
      "Epoch 29/30\n",
      "450/450 [==============================] - 0s 107us/step - loss: 0.1304 - acc: 0.9978 - val_loss: 0.4938 - val_acc: 0.7648\n",
      "Epoch 30/30\n",
      "450/450 [==============================] - 0s 106us/step - loss: 0.1254 - acc: 1.0000 - val_loss: 0.4932 - val_acc: 0.7648\n",
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:45 - Predictions:\n",
      "500/500 [==============================] - 0s 259us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:45 - Test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:45 - Performance:\n",
      "500/500 [==============================] - 0s 40us/step\n",
      "2018-12-10 22:57:45 - [score, accuracy]\n",
      "2018-12-10 22:57:45 - [0.4694198398590088, 0.8100000009536743]\n",
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:45 - Saving parameters: results/parameters/parameters_20181210225719_acc_0.810000.csv\n",
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:45 - Saving predictions: results/predictions/predictions_20181210225719_acc_0.810000.csv\n",
      "2018-12-10 22:57:45 - Done.\n",
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n",
      "2018-12-10 22:57:45 - STOP.\n",
      "2018-12-10 22:57:45 ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cargamos argumentos de prueba\n",
    "arguments = ['--model=1',\n",
    "             '--max_features=6000',\n",
    "             '--num_units=16',\n",
    "             '--dropout=0.2',\n",
    "             '--batch_size=200',             \n",
    "             '--epochs=30',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\n",
    "\"\"\"\n",
    "arguments = ['--model=1',\n",
    "             '--max_features=4000',\n",
    "             '--num_units=1',\n",
    "             '--dropout=0.5',\n",
    "             '--batch_size=200',             \n",
    "             '--epochs=70',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\"\"\"\n",
    "\n",
    "# Ejecutamos el procedimiento principal\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T04:10:46.640400Z",
     "start_time": "2018-09-22T04:09:09.824815Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cargamos argumentos de prueba\n",
    "arguments = ['--model=2',\n",
    "             '--max_features=6000',\n",
    "             '--num_units=3000',\n",
    "             '--dropout=0.2',\n",
    "             '--batch_size=200',             \n",
    "             '--epochs=30',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\n",
    "# Ejecutamos el procedimiento principal\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lucho\n",
    "# !Cargamos argumentos de prueba\n",
    "arguments = ['--model=1',\n",
    "             '--max_features=6000',\n",
    "             '--num_units=300',\n",
    "             '--dropout=0.2',\n",
    "             '--batch_size=200',             \n",
    "             '--epochs=30',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\n",
    "# Ejecutamos el procedimiento principal\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reporte \n",
    "\n",
    "__Responde al punto:__\n",
    "_Reportar los hyperparámetros y resultados de todos los modelos entrenados. Para esto, pueden utilizar una notebook o un archivo (pdf|md). Dentro de este reporte tiene que describir:_\n",
    "\n",
    "* _Hyperparámetros con los que procesaron el dataset: tamaño del vocabulario, normalizaciones, etc._\n",
    "* _Las decisiones tomadas al construir cada modelo: regularización, dropout, número y tamaño de las capas, optimizador._\n",
    "* _Proceso de entrenamiento: división del train/test, tamaño del batch, número de épocas, métricas de evaluación._ \n",
    "* _Seleccione los mejores hiperparámetros en función de su rendimiento. El proceso de entrenamiento debería ser el mismo\n",
    "para todos los modelos._\n",
    "* _(Punto estrella) Analizar si el clasificador está haciendo overfitting. Esto se puede determinar a partir del resultado del método fit._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Reporte de Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training samples 1500 (1500), test_samples 500 (500)\n",
    "2018-12-08 17:36:49 - x_train_vec - type: <class 'numpy.ndarray'>, shape:(1500, 35393)\n",
    "2018-12-08 17:36:50 - x_test_vec - type: <class 'numpy.ndarray'>, shape:(500, 22062)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Descripción de los Modelos Utilizados\n",
    "\n",
    "Se describen a continuación los modelos utilizados para el trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Descripción del Modelo 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Descripción del Modelo 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Descripción del Modelo 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Descripción del Modelo 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Descripción de Proceso de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Selección de los mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Análisis sobre el Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
