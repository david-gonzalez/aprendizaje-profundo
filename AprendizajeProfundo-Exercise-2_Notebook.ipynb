{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Archivo execise_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import keras.backend as K\n",
    "# K is just another name for the keras backend: tensorflow (or theaso,\n",
    "# if you are using a different backend).\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Embedding, Average, Lambda\n",
    "from keras.models import Sequential\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import FilteredFastText\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from tensorflow.python.keras.utils import np_utils   # for tf 1.3.1\n",
    "#from tensorflow.python.keras import utils as np_utils     # for tf 1.4.1\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "\n",
    "from itertools import repeat\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from printutils import print_message, print_new_process, print_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_args():\n",
    "    parser = argparse.ArgumentParser(description='Exercise 2')\n",
    "    # Here you have some examples of classifier parameters. You can add\n",
    "    # more arguments or change these if you need to.\n",
    "    parser.add_argument('--num_units', nargs='+', default=[100], type=int,\n",
    "                        help='Number of hidden units of each hidden layer.')\n",
    "    parser.add_argument('--dropout', nargs='+', default=[0.5], type=float,\n",
    "                        help='Dropout ratio for every layer.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Number of instances in each batch.')\n",
    "    parser.add_argument('--experiment_name', type=str, default=None,\n",
    "                        help='Name of the experiment, used in the filename'\n",
    "                             'where the results are stored.')\n",
    "    parser.add_argument('--embeddings_filename', type=str,\n",
    "                        help='Name of the file with the embeddings.')\n",
    "\n",
    "    # New parameters:    \n",
    "    parser.add_argument('--model', type=int, default=10, help='Number of model to run')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')\n",
    "    parser.add_argument('--shuffle', type=str, default='batch', help='Shuffle value')\n",
    "    parser.add_argument('--random_seed', type=int, default=10, help='Random seed number')\n",
    "    parser.add_argument('--verbose', type=int, default=1, help='Verbose info on screen')\n",
    "    \n",
    "    # parse parameters\n",
    "    if arguments == None:\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        args = parser.parse_args(arguments)\n",
    "\n",
    "    assert len(args.num_units) == len(args.dropout)\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = load_files('dataset/txt_sentoken', shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=42)\n",
    "\n",
    "    print('Training samples {}, test_samples {}'.format(\n",
    "        len(X_train), len(X_test)))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(instances, mapping):\n",
    "    \"\"\"Replaces the words in instances with their index in mapping.\n",
    "\n",
    "    Args:\n",
    "        instances: a list of text instances.\n",
    "        mapping: an dictionary from words to indices.\n",
    "\n",
    "    Returns:\n",
    "        A matrix with shape (n_instances, max_text_length).\"\"\"\n",
    "    word_indices = []\n",
    "    for instance in instances:\n",
    "        word_indices.append([mapping[word.decode('utf-8')]\n",
    "                             for word in instance.split()])\n",
    "    # Check consistency\n",
    "    assert len(instances[0].split()) == len(word_indices[0])\n",
    "\n",
    "    # Pad the sequences to obtain a matrix instead of a list of lists.\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    return pad_sequences(word_indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load params\n",
    "    args = read_args()\n",
    "\n",
    "    # Get datetime of experiment.\n",
    "    experiment_number = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Configurar semilla para facilitar reproducibilidad.\n",
    "    np.random.seed(args.random_seed)\n",
    "    \n",
    "    # Load dataset    \n",
    "    X_train, X_test, y_train, y_test_orginal = load_dataset()\n",
    "\n",
    "    # Convert the labels to categorical\n",
    "    num_classes = 2 \n",
    "    y_train_cat = np_utils.to_categorical(y_train, num_classes)    \n",
    "    y_test_cat = np_utils.to_categorical(y_test_orginal, num_classes)    \n",
    "\n",
    "    # Verify balance of labels in train and test\n",
    "    # Print count frequency of labels in train and test. \n",
    "    #print(np.bincount(y_train))\n",
    "    #print(np.bincount(y_test_orginal))\n",
    "    \n",
    "    # Load the filtered FastText word vectors, using only the vocabulary in\n",
    "    # the movie reviews dataset\n",
    "    with open(args.embeddings_filename, 'rb') as model_file:\n",
    "        filtered_fasttext = pickle.load(model_file)\n",
    "\n",
    "    # The next thing to do is to choose how we are going to represent our\n",
    "    # training matrix. Each review must be translated into a single vector.\n",
    "    # This means we have to combine, somehow, the word vectors of each\n",
    "    # word in the review. Some options are:\n",
    "    #  - Take the average of all vectors.\n",
    "    #  - Take the minimum and maximum value of each feature.\n",
    "    # All these operations are vectorial and easier to compute using a GPU.\n",
    "    # Then, it is better to put them inside the Keras model.\n",
    "\n",
    "    # The Embedding layer will be quite handy in solving this problem for us.\n",
    "    # To use this layer, the input to the network has to be the indices of the\n",
    "    # words on the embedding matrix.\n",
    "    \n",
    "    \n",
    "    X_train_vectorized = transform_input(X_train, filtered_fasttext.word2index)\n",
    "    X_test_vectorized = transform_input(X_test, filtered_fasttext.word2index)\n",
    "\n",
    "\n",
    "    #word_indices = []\n",
    "    #for instance in X_train:\n",
    "    #    word_indices.append([filtered_fasttext.word2index[word.decode('utf-8')]\n",
    "    #                         for word in instance.split()])\n",
    "    # Check consistency\n",
    "    #assert len(X_train[0].split()) == len(word_indices[0])\n",
    "    #word_indices = np.array(word_indices)\n",
    "    \n",
    "    # The input is ready, start the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        filtered_fasttext.wv.shape[0],  # Vocabulary size\n",
    "        filtered_fasttext.wv.shape[1],  # Embedding size\n",
    "        weights=[filtered_fasttext.wv],  # Word vectors\n",
    "        trainable=False  # This indicates the word vectors must not be changed\n",
    "                         # during training.\n",
    "    ))\n",
    "    # The output here has shape\n",
    "    #     (batch_size (?), words_in_reviews (?), embedding_size)\n",
    "    # To use a Dense layer, the input must have only 2 dimensions. We need to\n",
    "    # create a single representation for each document, combining the word\n",
    "    # embeddings of the words in the intance.\n",
    "    # For this, we have to use a Tensorflow (K) operation directly.\n",
    "    # The operation we need to do is to take the average of the embeddings\n",
    "    # on the second dimension. We wrap this operation on a Lambda\n",
    "    # layer to include it into the model.\n",
    "    model.add(Lambda(lambda xin: K.mean(xin, axis=1), name='embedding_average'))\n",
    "    # Now the output shape is (batch_size (?), embedding_size)\n",
    "\n",
    "    # TODO 2: Finish the Keras model\n",
    "    # Add all the layers\n",
    "    # ...\n",
    "    # model.compile(...)\n",
    "    #input_size = X_train.shape[1]\n",
    "    #model.add(Dense(args.num_units[0], input_shape=(input_size,)))    \n",
    "\n",
    "    model.add(Dense(args.num_units[0]))    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(args.dropout[0]))    \n",
    "    \n",
    "    #---\n",
    "    #model.add(Dense(args.num_units[0]))    \n",
    "    #model.add(Activation('relu'))\n",
    "    #---\n",
    "\n",
    "\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizers.Adagrad(), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Show model info on screen\n",
    "    print_message('MODEL:', args )\n",
    "    print_message( model.summary(), args )\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # TODO 3: Fit the model\n",
    "    # hitory = model.fit(batch_size=??, ...)\n",
    "    print_new_process('Fit:', args)\n",
    "    history = model.fit(X_train_vectorized, y_train_cat,\n",
    "                        batch_size=args.batch_size,\n",
    "                        epochs=args.epochs,\n",
    "                        shuffle=args.shuffle,\n",
    "#                        validation_data=(X_test_vectorized, y_test_cat), \n",
    "                        verbose=1,\n",
    "             )\n",
    "    \n",
    "    # TODO 4: Evaluate the model, calculating the metrics.\n",
    "    # Option 1: Use the model.evaluate() method. For this, the model must be\n",
    "    # already compiled with the metrics.\n",
    "    # performance = model.evaluate(transform_input(X_test), y_test)\n",
    "\n",
    "    # Option 2: Use the model.predict() method and calculate the metrics using\n",
    "    # sklearn. We recommend this, because you can store the predictions if\n",
    "    # you need more analysis later. Also, if you calculate the metrics on a\n",
    "    # notebook, then you can compare multiple classifiers.\n",
    "    # predictions = ...\n",
    "    # performance = ...\n",
    "\n",
    "    \n",
    "    print_new_process('Predictions:',args)\n",
    "    predictions = model.predict_classes(X_test_vectorized, verbose=1)\n",
    "    \n",
    "    if args.verbose == 1:\n",
    "        display( str(list(predictions)))\n",
    "\n",
    "    print_new_process('Test:',args)\n",
    "    if args.verbose == 1:        \n",
    "        display( str(list(y_test_orginal)))\n",
    " \n",
    "    print_new_process('Performance:',args)\n",
    "    score, accuracy = model.evaluate(X_test_vectorized, y_test_cat)\n",
    "    print_message( '[score, accuracy]', args )\n",
    "    print_message( [score, accuracy], args )\n",
    "    \n",
    "    # TODO 5: Save the results.\n",
    "    # ...\n",
    "    parameters_path = 'results/parameters/'\n",
    "    if not os.path.exists(parameters_path):\n",
    "        os.makedirs(parameters_path)\n",
    "    predictions_path = 'results/predictions/'\n",
    "    if not os.path.exists(predictions_path):\n",
    "        os.makedirs(predictions_path)\n",
    "\n",
    "    parameters_filename = 'results/parameters/parameters_{0}_acc_{1:.6f}.csv'.format( experiment_number, accuracy )\n",
    "    print_new_process('Saving parameters: {}'.format(parameters_filename),args)  \n",
    "    \n",
    "    parameters_df = pandas.DataFrame(columns=['Parameter','Value'])\n",
    "    for k,v in sorted(vars(args).items()):\n",
    "        row = pandas.Series([str(k), str(v)], index=['Parameter', 'Value'])\n",
    "        parameters_df = parameters_df.append(row,ignore_index=True)\n",
    "    parameters_df.to_csv(parameters_filename, index=False )\n",
    "    \n",
    "    predictions_filename = 'results/predictions/predictions_{0}_acc_{1:.6f}.csv'.format( experiment_number, accuracy )\n",
    "    print_new_process('Saving predictions: {}'.format(predictions_filename),args)  \n",
    "    predictions_df = pandas.DataFrame(y_test_orginal, columns=['true_label'])\n",
    "    predictions_df.loc[:, 'predicted'] = predictions\n",
    "    predictions_df.to_csv( predictions_filename, index=False )\n",
    "    \n",
    "    print_message('Done.',args)    \n",
    "    print_end('STOP.',args)    \n",
    "    \n",
    "    \n",
    "    # One way to store the predictions:\n",
    "    \"\"\"\n",
    "    results = pandas.DataFrame(y_test_orginal, columns=['true_label'])\n",
    "    results.loc[:, 'predicted'] = predictions\n",
    "    results.to_csv('predicitions_{}.csv'.format(args.experiment_name),\n",
    "                   index=False)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples 1500, test_samples 500\n",
      "2018-09-22 09:02:06 - MODEL:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         15276000  \n",
      "_________________________________________________________________\n",
      "embedding_average (Lambda)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 15,366,902\n",
      "Trainable params: 90,902\n",
      "Non-trainable params: 15,276,000\n",
      "_________________________________________________________________\n",
      "2018-09-22 09:02:06 - None\n",
      "2018-09-22 09:02:06 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:06 - Fit:\n",
      "Epoch 1/15\n",
      "1500/1500 [==============================] - 1s 456us/step - loss: 0.7194 - acc: 0.5160\n",
      "Epoch 2/15\n",
      "1500/1500 [==============================] - 1s 420us/step - loss: 0.6888 - acc: 0.5387\n",
      "Epoch 3/15\n",
      "1500/1500 [==============================] - 1s 503us/step - loss: 0.6863 - acc: 0.5493\n",
      "Epoch 4/15\n",
      "1500/1500 [==============================] - 1s 542us/step - loss: 0.6865 - acc: 0.5427\n",
      "Epoch 5/15\n",
      "1500/1500 [==============================] - 1s 596us/step - loss: 0.6876 - acc: 0.5227\n",
      "Epoch 6/15\n",
      "1500/1500 [==============================] - 1s 554us/step - loss: 0.6866 - acc: 0.5373\n",
      "Epoch 7/15\n",
      "1500/1500 [==============================] - 1s 593us/step - loss: 0.6829 - acc: 0.5467\n",
      "Epoch 8/15\n",
      "1500/1500 [==============================] - 1s 582us/step - loss: 0.6833 - acc: 0.5460\n",
      "Epoch 9/15\n",
      "1500/1500 [==============================] - 1s 609us/step - loss: 0.6812 - acc: 0.5600\n",
      "Epoch 10/15\n",
      "1500/1500 [==============================] - 1s 595us/step - loss: 0.6839 - acc: 0.5393\n",
      "Epoch 11/15\n",
      "1500/1500 [==============================] - 1s 568us/step - loss: 0.6812 - acc: 0.5567\n",
      "Epoch 12/15\n",
      "1500/1500 [==============================] - 1s 563us/step - loss: 0.6823 - acc: 0.5573\n",
      "Epoch 13/15\n",
      "1500/1500 [==============================] - 1s 565us/step - loss: 0.6818 - acc: 0.5620\n",
      "Epoch 14/15\n",
      "1500/1500 [==============================] - 1s 594us/step - loss: 0.6820 - acc: 0.5533\n",
      "Epoch 15/15\n",
      "1500/1500 [==============================] - 1s 588us/step - loss: 0.6796 - acc: 0.5647\n",
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:19 - Predictions:\n",
      "500/500 [==============================] - 0s 524us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:19 - Test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:19 - Performance:\n",
      "500/500 [==============================] - 0s 583us/step\n",
      "2018-09-22 09:02:19 - [score, accuracy]\n",
      "2018-09-22 09:02:19 - [0.6960780019760132, 0.5500000009536743]\n",
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:19 - Saving parameters: results/parameters/parameters_20180922090205_acc_0.550000.csv\n",
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:19 - Saving predictions: results/predictions/predictions_20180922090205_acc_0.550000.csv\n",
      "2018-09-22 09:02:19 - Done.\n",
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n",
      "2018-09-22 09:02:19 - STOP.\n",
      "2018-09-22 09:02:19 ------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# cargamos argumentos de prueba\n",
    "\"\"\"\n",
    "arguments = ['--num_units=512',\n",
    "             '--dropout=0.5',\n",
    "             '--batch_size=100',             \n",
    "             '--epochs=15',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\"\"\"\n",
    "\n",
    "arguments = ['--num_units=300',\n",
    "             '--dropout=0.02',\n",
    "             '--batch_size=100', \n",
    "             '--embeddings_filename=filtered_wiki-news-300d-50k.vec',\n",
    "             '--epochs=15',\n",
    "             '--shuffle=batch',\n",
    "             '--random_seed=10',\n",
    "             '--verbose=1'\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutamos el procedimiento principal\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
